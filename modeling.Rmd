---
title: "Predictive Modeling"
output: html_notebook
---

This notebook provides the code for classification.

Our proposed method first split the 20 samples into training (n=14) and testing (n=6) set. We first trained a preliminary classifier on the training set with scanner data that categorized the samples into three classes (good, bad and uncertain). We made predictions on the test data, then the test samples that were classified as "uncertain" were taken to a more refined classifier trained with high resolution VR measurements. The "uncertain" samples were then predicted again to either "good" or "bad". 

## Label the data with ground truth

We first give all the samples of a ground truth label of either good or bad using K-means clustering. 
```{r,warning=FALSE}
# read the quality metrics
qualitydf<-read.csv("qualitydf.csv",header = T)

# clustering with profile resid and roughness to give the samples a ground "truth"
set.seed(4747)
library(ggplot2)
quality_cluster<-kmeans(qualitydf[,c(1,2)],2)

# label the cluster
cluster<-quality_cluster$cluster
for(i in 1:length(cluster)){
  cluster[i]<-ifelse(cluster[i]==2,"bad","good")
}

# factorize the cluster label and create a big data frame
ground_truth<-as.factor(cluster)
bigdf<-cbind(qualitydf,ground_truth)

# plot the clustering result

## ggplot
ggplot(bigdf,aes(x=texture,y=profile_resid,shape=as.factor(height),color=cluster))+geom_point()+coord_flip()+theme_classic()

# normal plot
goodmeanX<-mean(bigdf[which(bigdf$ground_truth=="good"),]$profile_resid)
badmeanX<-mean(bigdf[which(bigdf$ground_truth=="bad"),]$profile_resid)
goodmeanY<-mean(bigdf[which(bigdf$ground_truth=="good"),]$texture)
badmeanY<-mean(bigdf[which(bigdf$ground_truth=="bad"),]$texture)

plot(
 bigdf$profile_resid,bigdf$texture,col=as.factor(bigdf$ground_truth),ylab="Roughness (%)",xlab="Profile Residual (um^2)",xlim=c(2,16),pch=20
)
points(goodmeanX,goodmeanY,cex=1.5,col="red",pch=18)
points(badmeanX,badmeanY,cex=1.5,col="black",pch=18)
legend("topright", legend=c("Good", "Bad","Good mean","Bad mean"),pch=c(20,20,18,18),
       col=c("red", "black","red","black"),cex=0.8)
```
The clustering result shows that nGood = nBad = 10

## Scanner classifier only

We now train a classifier with only scanner data. We use this classifier as a comparison to our proposed method to evaluate their predictive strength.

```{r,warning=FALSE}
# read the scanner height data
library(readr)
folder <- "measurements/scanner/"
file_list <- list.files(path = folder, pattern = "*.obj")

scannerdf<-data.frame()
for (i in 1:length(file_list)){
  scanner<-read_delim(paste(folder, file_list[i],sep='')," ", escape_double = FALSE, col_names = FALSE,
    trim_ws = TRUE, skip = 3)[,-1]
  name<-paste("scanner-",substr(file_list[i],start=1,stop=nchar(file_list[i])-4),sep ='')
  assign(name, scanner) 
  
  X<-scanner$X2
  Y<-scanner$X3
  Z<-scanner$X4
  
  # trim X, Y and Z that are out of range
  xTrim<-which(abs(X)<=32)
  Y<-Y[xTrim]
  Z<-Z[xTrim]
  X<-X[xTrim]

  yTrim<-which(abs(Y)<=32)
  Z<-Z[yTrim]
  X<-X[yTrim]
  Y<-Y[yTrim]
  
  # put Z of each sample into the rows of the df
  scannerdf<-rbind(scannerdf,Z)
}

# omit NAs with zero
scannerdf[is.na(scannerdf)]<-0
dim(scannerdf)
```

```{r}
# split into train and test by 7:3
# sample 3 from good and 3 from bad
set.seed(4747)
goodind<-sample(which(bigdf$ground_truth=="good"),3,replace=FALSE)
badind<-sample(which(bigdf$ground_truth=="bad"),3,replace=FALSE)
testind<-c(goodind,badind)
bigdf[testind,]
```

```{r}
# extract pca from scanner train set
scanner_mat_train<-as.matrix(scannerdf)[-testind,]
pca<-prcomp(scanner_mat_train,center=TRUE)
summary(pca)
scanner_pca_train<-pca$x[,1:4]
colnames(scanner_pca_train)<-c("scanner_PC1","scanner_PC2","scanner_PC3","scanner_PC4")

# extract pca from scanner test set
scanner_mat_test<-as.matrix(scannerdf)[testind,]
scanner_pca_test<-prcomp(scanner_mat_test,center=TRUE)$x[,1:4]
colnames(scanner_pca_test)<-c("scanner_PC1","scanner_PC2","scanner_PC3","scanner_PC4")
```


```{r}
# create the training data frame
train_df<-data.frame(cbind(scanner_pca_train,bigdf[-testind,][,-c(1,2)]))
test_df<-data.frame(cbind(scanner_pca_test,bigdf[testind,][,-c(1,2)]))
```

### Train a Random Forest
```{r}
# using only scanner pca
library(Metrics)

# 3-fold cv
control<-trainControl(method="cv",number=3,summaryFunction=twoClassSummary,classProbs = TRUE)
seed <- 47
metric <- "ROC"  # use ROC metric
set.seed(seed)
mtry <- 1:7 
tunegrid <- expand.grid(.mtry=mtry)
rf_pca <- train(ground_truth~., data=train_df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control,preProcess = c("scale", "center"))
rf_pca
plot(varImp(rf_pca))
pred_scanner<-predict(rf_pca,test_df)
confusionMatrix(predict(rf_pca,test_df),test_df$ground_truth)
```
The most optimal cv ROC score is 0.5648. The test accuracy is 0.6667.

```{r}
# show variable importance
varImp(rf_pca)
```


## Proposed Method using both scanner and high resolution
### Training the first classifier
The first classifier is trained with only scanner data. We first label the training samples with good, bad or uncertain. Then we predict the test samples. This is a multiclass classification.
```{r}
# split into train and test 
new_train<-qualitydf[-testind,]
new_test<-qualitydf[testind,]

# cluster the training samples into good or bad
set.seed(47)
clustering<-kmeans(new_train[,c(1,2)],3)
cluster<-clustering$cluster
for(i in 1:length(cluster)){
  cluster[i]<-ifelse(cluster[i]==3,"good",ifelse(cluster[i]==2,"uncertain","bad"))
}
cluster<-as.factor(cluster)
new_train<-cbind(new_train,cluster)

# plot the clustering result
ggplot(new_train,aes(x=texture,y=profile_resid,shape=as.factor(height),color=cluster))+geom_point()+coord_flip()+theme_classic()
```

nGood:nBad:nUncertain = 3:4:7

```{r}
# a multiclass summary function that will be used for multiclass classification

library(compiler)
multiClassSummary <- cmpfun(function (data, lev = NULL, model = NULL){

#Load Libraries
require(Metrics)
require(caret)

#Check data
if (!all(levels(data[, "pred"]) == levels(data[, "obs"]))) 
stop("levels of observed and predicted data do not match")

#Calculate custom one-vs-all stats for each class
prob_stats <- lapply(levels(data[, "pred"]), function(class){

#Grab one-vs-all data for the class
pred <- ifelse(data[, "pred"] == class, 1, 0)
obs <- ifelse(data[, "obs"] == class, 1, 0)
prob <- data[,class]

#Calculate one-vs-all AUC and logLoss and return
cap_prob <- pmin(pmax(prob, .000001), .999999)
prob_stats <- c(auc(obs, prob), logLoss(obs, cap_prob))
names(prob_stats) <- c('ROC', 'logLoss')
return(prob_stats) 
})
prob_stats <- do.call(rbind, prob_stats)
rownames(prob_stats) <- paste('Class:', levels(data[, "pred"]))

#Calculate confusion matrix-based statistics
CM <- confusionMatrix(data[, "pred"], data[, "obs"])

#Aggregate and average class-wise stats
#Todo: add weights
class_stats <- cbind(CM$byClass, prob_stats)
class_stats <- colMeans(class_stats)

#Aggregate overall stats
overall_stats <- c(CM$overall)

#Combine overall with class-wise stats and remove some stats we don't want 
stats <- c(overall_stats, class_stats)
stats <- stats[! names(stats) %in% c('AccuracyNull', 
'Prevalence', 'Detection Prevalence')]

#Clean names and return
names(stats) <- gsub('[[:blank:]]+', '_', names(stats))
return(stats)
})
```

```{r} 
# train the first classifier
scanner_train_df<-data.frame(cbind(scanner_pca_train,new_train[,-c(1,2)]))
scanner_test_df<-data.frame(cbind(scanner_pca_test,new_test[,-c(1,2)]))

# 
control <- trainControl(method="cv",number=3,sampling = "up",summaryFunction=multiClassSummary,classProbs = TRUE)
seed <- 4747
metric <- "ROC"
set.seed(seed)
mtry <- 1:7
tunegrid <- expand.grid(.mtry=mtry)
rf_scanner <- train(cluster~., data=scanner_train_df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control,preProcess = c("scale", "center"))
rf_scanner
predict(rf_scanner,scanner_test_df)
bigdf[testind,]$ground_truth
plot(varImp(rf_scanner))
```
The optimal cv ROC score is 0.5388. All the test samples are predicted as uncertain.

### Training the second classifier
```{r}
# read in the VR high resolution measurement matrices 
folder <- "measurements/matrix/"
file_list <- list.files(path = folder, pattern = "*.csv")

mat_pca<-matrix(0,nrow=length(file_list),ncol=1520*1628)
namelst<-list()
for (i in 1:length(file_list)){
    df<- read.csv(paste(folder, file_list[i],sep=''),header=F,skip=23)
    df[is.na(df)]<-0
    #imputedf<-DMwR::knnImputation(df,k=5)
    name<-substr(file_list[i],start=11,stop=nchar(file_list[i])-17)
    namelst<-c(namelst,name)
    if (dim(df)[2]>1628){      # fix a minor issue where one matrix is reversed
      df<-df[1:1520,1:1628]
    }
    if (dim(df)[2]==1520){
      df<-t(df)
    }
    assign(name, df) 
    mat_pca[i,]<-as.vector(as.matrix(df))
}
length(file_list)
dim(mat_pca)
```

```{r}
# export the high resolution matrices for matlab umpca code
trainind<-seq(1,20,by=1)[-testind]
# clear previous files before writing the new ones
file.remove(file.path("umpca/matrices/train/", list.files("umpca/matrices/train/"))) 

# export train matrices
for (i in trainind){
  mat<-matrix(mat_pca[i,],1520,1628);
  write.csv(mat,paste("umpca/matrices/train/",namelst[i],".csv",sep=''))
}

# export test matrices
file.remove(file.path("umpca/matrices/test/", list.files("umpca/matrices/test/"))) 
for (i in testind){
  mat<-matrix(mat_pca[i,],1520,1628);
  write.csv(mat,paste("umpca/matrices/test/",namelst[i],".csv",sep=''))
}
```

```{r}
# load the umpca components from matlab
newfea_train<-read.csv("umpca/newfea_train.csv",header=FALSE)
num<-1:dim(newfea_train)[2]
names(newfea_train)<-paste("EMP",num,sep='')

newfea_test<-read.csv("umpca/newfea_test.csv",header=FALSE)
num<-1:dim(newfea_test)[2]
names(newfea_test)<-paste("EMP",num,sep='')
```

We trained a two-class classifier on the train set with high resolution features.
```{r}
# cluster the training samples into 2 classes
set.seed(47)
second_cluster<-kmeans(qualitydf[-testind,][,c(1,2)],2)

# label the cluster
cluster<-second_cluster$cluster
for(i in 1:length(cluster)){
  cluster[i]<-ifelse(cluster[i]==2,"good","bad")
}
cluster<-as.factor(cluster)
vr_df<-cbind(qualitydf[-testind,],cluster)
```

```{r}
# run a pca on VR high resolution data
vr_pca_train<-prcomp(mat_pca[-testind,],center = TRUE)$x[,1:4]
vr_pca_test<-prcomp(mat_pca[testind,],center = TRUE)$x[,1:4]
```

First train a classifier with pca features.
```{r}
# rf with pca
vr_train_df<-data.frame(cbind(vr_pca_train,vr_df[,-c(1,2)]))
vr_test_df<-data.frame(cbind(vr_pca_test,new_test[,-c(1,2)]))

control<-trainControl(method="cv",number=3,sampling = "up",summaryFunction=twoClassSummary,classProbs = TRUE)
seed <- 4747
#control<-trainControl(method="oob")
#metric<-"Accuracy"
metric <- "ROC"
set.seed(seed)
mtry <- 1:7
tunegrid <- expand.grid(.mtry=mtry)
rf_vr <- train(cluster~., data=vr_train_df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control,preProcess = c("scale", "center"))
rf_vr
confusionMatrix(predict(rf_vr,vr_test_df),bigdf[testind,]$ground_truth)
```
The optimal cv ROC score is 0.8889. Test accuracy is 0.3333.

Next we train the second classifier with UMPCA features
```{r,include=FALSE}
# create the umpca training data frame
umpca_train_df<-data.frame(cbind(vr_df,newfea_train))[,-c(1,2)]
umpca_test_df<-data.frame(cbind(new_test,newfea_test))[,-c(1,2)]
```

```{r}
# rf with umpca
control<-trainControl(method="cv",number=3,sampling = "up",summaryFunction=twoClassSummary,classProbs = TRUE)
#control<-trainControl(method="oob")
seed <- 47
metric <- "ROC"
set.seed(seed)
mtry <- 1:7
tunegrid <- expand.grid(.mtry=mtry)
rf_vr_umpca <- train(cluster~., data=umpca_train_df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control,preProcess = c("scale", "center"))
rf_vr_umpca
plot(varImp(rf_vr_umpca))
pred_umpca<-predict(rf_vr_umpca,umpca_test_df)
confusionMatrix(pred_umpca,bigdf[testind,]$ground_truth)
```
The optimal cv ROC is 0.6111 (lower than pca cv roc yet higher than the control classifier roc). Yet the test accuracy is high (0.8333).


